{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 — Advanced Training\n",
    "\n",
    "In this tutorial we will cover some more advanced parameters for a realistic training of a potential.\n",
    "\n",
    "In particular, we will cover:\n",
    "* Tranining with forces\n",
    "* Regularization\n",
    "* Decaying learning rate\n",
    "* Validating during training\n",
    "* Computing Gvectors during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PANNA is installed correctly\n"
     ]
    }
   ],
   "source": [
    "# We start with the standard setup\n",
    "import os\n",
    "\n",
    "# Specify the absolute path to PANNA (or leave this relative path)\n",
    "panna_dir = os.path.abspath('../..')\n",
    "\n",
    "# In case you need to mount the drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# panna_dir = '/content/drive/MyDrive/your_path_to_panna'\n",
    "\n",
    "# Cleaning up path for command line\n",
    "panna_cmdir = panna_dir.replace(' ', '\\ ')\n",
    "\n",
    "# Check if PANNA is installed, otherwise install it\n",
    "try:\n",
    "  import panna\n",
    "  print(\"PANNA is installed correctly\")\n",
    "except ModuleNotFoundError:\n",
    "  print(\"PANNA not found, attempting to install\")\n",
    "  !pip install panna_cmdir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 — Traning with forces\n",
    "\n",
    "In part 2 we have performed training to learn the energy of the reference configurations.\n",
    "But of course we can compute the force on each atom by differentiating the energy with respect to its position, and if we have a ground truth for these forces, we can use this information to improve our training.\n",
    "\n",
    "In order to compute the derivative with respect to position, we need to know the derivative with respect to each component of each descriptor $G_j$:\n",
    "$$F_i=\\frac{\\partial E}{\\partial x_i}=\\sum_j \\frac{\\partial E}{\\partial G_j}\\frac{\\partial G_j}{\\partial x_i}.$$\n",
    "These terms will be added to the ``tfr`` in the data creation pipeline by including the flag ``include_derivatives = True`` in the ``[SYMMETRY_FUNCTION]`` card of the descriptor calculator and in the ``[CONTENT_INFORMATION]`` card of the packer.\n",
    "Additionally, we can store all the possible derivatives, or only the elements different from zero (you can imagine that in a large cell each atom will only affect the descriptor of the few atoms in a cutoff sized sphere around itself). To switch from storing all data to the sparse format, the flag ``sparse_derivatives = True`` can be included along with the previous one.\n",
    "\n",
    "For this tutorial, we have already created ``tfr`` files of a few configurations for you, with derivatives stored in a dense format. Please note that information about the derivative can take up a lot of space; for this reason we have limited this dataset to only a few water configurations. While this is not enough for any meaningful training, it is sufficient to showcase this training option.\n",
    "\n",
    "Once we have the derivatives in the data, to use them (and reference forces) in training, it is sufficient to add the keyword ``forces_cost`` to the training parameters, with a value greater than zero.\n",
    "Let's look at a sample input file, then run this short training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO_INFORMATION]\r\n",
      "data_dir = ./tutorial_data/train_force\r\n",
      "train_dir = ./my_train_force\r\n",
      "log_frequency = 10\r\n",
      "save_checkpoint_steps = 50\r\n",
      "\r\n",
      "[DATA_INFORMATION]\r\n",
      "atomic_sequence = H, O\r\n",
      "output_offset = -13.49, -562.1\r\n",
      "\r\n",
      "[TRAINING_PARAMETERS]\r\n",
      "batch_size = 10\r\n",
      "learning_rate = 0.001\r\n",
      "steps_per_epoch = 50\r\n",
      "max_epochs = 2\r\n",
      "forces_cost = 0.1\r\n",
      "\r\n",
      "[DEFAULT_NETWORK]\r\n",
      "g_size = 128\r\n",
      "architecture = 128:32:1\r\n",
      "trainable = 1:1:1\r\n"
     ]
    }
   ],
   "source": [
    "!cat {panna_cmdir+'/doc/tutorial/input_files/mytrain_force.ini'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 11:02:00.173646: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-04-25 11:02:00.173705: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-04-25 11:02:00.173713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO - \n",
      "    ____   _    _   _ _   _    _           \n",
      "   |  _ \\ / \\  | \\ | | \\ | |  / \\     \n",
      "   | |_) / _ \\ |  \\| |  \\| | / _ \\     \n",
      "   |  __/ ___ \\| |\\  | |\\  |/ ___ \\    \n",
      "   |_| /_/   \\_\\_| \\_|_| \\_/_/   \\_\\ \n",
      "\n",
      " Properties from Artificial Neural Network Architectures\n",
      "\n",
      "INFO - reading ./input_files/mytrain_force.ini\n",
      "INFO - Found a default network!\n",
      "INFO - This network size will be used as default for all species unless specified otherwise\n",
      "Epoch 1/2\n",
      "50/50 [==============================] - 4s 13ms/step - tot_st: 50.0000 - MAE/at: 93.0316 - F_MAE: 0.7981\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 1s 11ms/step - tot_st: 100.0000 - MAE/at: 91.7953 - F_MAE: 0.7782\n"
     ]
    }
   ],
   "source": [
    "!cd {panna_cmdir+'/doc/tutorial/'}; python {panna_cmdir+'/src/panna/train.py'} --config ./input_files/mytrain_force.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the MAE over the forces is also reported during training (and in the metrics file and tensorboard).\n",
    "\n",
    "Please note that training with forces can be considerably slower than training with energy only. However, it allows us to produce considerably more accurate models when forces are important, e.g. for use as an interatomic potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 — Other training options\n",
    "\n",
    "We summarize here other options that can be useful during a real training.\n",
    "\n",
    "#### Regularization\n",
    "As in many neural network applications, it can be beneficial to add a small contribution to the loss function that keeps weights from growing. This is typically called a L1 (if it employs the ablsolute value of the weights) or L2 (if it employs the squares) regularization.\n",
    "\n",
    "In PANNA, we can introduce L1 regularization with the keywords ``wscale_l1`` for weights and ``bscale_l1`` for biases (and similar for l2) in the training parameters, specifying the small weight to use for this correction.\n",
    "\n",
    "#### Decaying learning rate\n",
    "When training a model, and especially when finalizing it, it can be useful to gradually reduce the learning rate. In PANNA, we can employ an exponentially decreasing learning rate following the equation:\n",
    "$$\\alpha(t)=\\alpha(0) r^{t/\\tau}$$\n",
    "where $t$ represents the training step.\n",
    "To use this, in the training parameters we need to set ``learning_rate_constant`` to ``False``, and we can set the value of $\\alpha_0$ as we would normally set the ``learning_rate``, $r$ with the keyword ``learning_rate_decay_factor`` and $\\tau$ with the keyword ``learning_rate_decay_step``.\n",
    "\n",
    "#### Metrics\n",
    "If we want to track a metric different from the MAE (or in addition to it) from the command line during training, we can use the keyword ``metrics`` in the io_information, followed by a comma separated list of the following values: ``MAE``, ``RMSE`` or ``loss``. The loss option reports all components the contribute to the loss driving the training.\n",
    "\n",
    "#### Validation during training\n",
    "At the end of each epoch (the one we set in the training input, not necessarily as imposed by the dataset), we can ask PANNA to automatically evaluate the model on a small set of validation examples. While this set is typically smaller than the full training set, it can be useful to give us an idea whether our model is overfitting to the tranining set, or the error is decreasing also over unseen examples. To enable this feature, a new card called ``[VALIDATION_OPTIONS]`` should be added, including at least the keyword ``data_dir`` indicating the location of the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "We can now look at a more complete training input file to see all of these options in place (to keep computational cost at a minimum, we go back to a simple training without forces, we will modify the original input file with the new options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO_INFORMATION]\r\n",
      "data_dir = ./tutorial_data/train\r\n",
      "train_dir = ./my_adv_train\r\n",
      "log_frequency = 100\r\n",
      "save_checkpoint_steps = 500\r\n",
      "metrics = RMSE,loss\r\n",
      "\r\n",
      "[DATA_INFORMATION]\r\n",
      "atomic_sequence = H, C, O, N\r\n",
      "output_offset = -13.62, -1029.41, -2041.84, -1484.87\r\n",
      "\r\n",
      "[TRAINING_PARAMETERS]\r\n",
      "batch_size = 20\r\n",
      "steps_per_epoch = 100\r\n",
      "max_epochs = 10\r\n",
      "wscale_l1 = 1e-4\r\n",
      "bscale_l1 = 1e-4\r\n",
      "learning_rate_constant = False\r\n",
      "learning_rate = 0.01\r\n",
      "learning_rate_decay_factor = 0.1\r\n",
      "learning_rate_decay_step = 200\r\n",
      "\r\n",
      "[DEFAULT_NETWORK]\r\n",
      "g_size = 384\r\n",
      "architecture = 128:32:1\r\n",
      "trainable = 1:1:1\r\n",
      "\r\n",
      "[VALIDATION_OPTIONS]\r\n",
      "data_dir = ./tutorial_data/validate\r\n"
     ]
    }
   ],
   "source": [
    "!cat {panna_cmdir+'/doc/tutorial/input_files/my_adv_train.ini'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set an L1 regularization equal to 0.0001, we decay the learning rate of 0.1 every 200 steps, i.e. from 0.01 to 1e-7, we want to track the RMSE and all loss components, and we will validate on the set used for validation in the previous tutorial.\n",
    "\n",
    "We can now run this small training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-24 17:58:12.628080: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-04-24 17:58:12.628126: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-04-24 17:58:12.628133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO - \n",
      "    ____   _    _   _ _   _    _           \n",
      "   |  _ \\ / \\  | \\ | | \\ | |  / \\     \n",
      "   | |_) / _ \\ |  \\| |  \\| | / _ \\     \n",
      "   |  __/ ___ \\| |\\  | |\\  |/ ___ \\    \n",
      "   |_| /_/   \\_\\_| \\_|_| \\_/_/   \\_\\ \n",
      "\n",
      " Properties from Artificial Neural Network Architectures\n",
      "\n",
      "INFO - reading ./input_files/my_adv_train.ini\n",
      "INFO - Found a default network!\n",
      "INFO - This network size will be used as default for all species unless specified otherwise\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 5s 11ms/step - tot_st: 100.0000 - RMSE/at: 0.2134 - loss: 5.1222 - e_loss: 4.8005 - reg_loss: 0.3216 - val_RMSE/at: 0.0423 - val_e_loss: 0.0224\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 6ms/step - tot_st: 200.0000 - RMSE/at: 0.0429 - loss: 0.2611 - e_loss: 0.0272 - reg_loss: 0.2339 - val_RMSE/at: 0.0338 - val_e_loss: 0.0150\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 8ms/step - tot_st: 300.0000 - RMSE/at: 0.0402 - loss: 0.2499 - e_loss: 0.0243 - reg_loss: 0.2256 - val_RMSE/at: 0.0346 - val_e_loss: 0.0148\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 11ms/step - tot_st: 400.0000 - RMSE/at: 0.0398 - loss: 0.2466 - e_loss: 0.0236 - reg_loss: 0.2229 - val_RMSE/at: 0.0358 - val_e_loss: 0.0179\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 7ms/step - tot_st: 500.0000 - RMSE/at: 0.0391 - loss: 0.2451 - e_loss: 0.0231 - reg_loss: 0.2220 - val_RMSE/at: 0.0338 - val_e_loss: 0.0149\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 6ms/step - tot_st: 600.0000 - RMSE/at: 0.0386 - loss: 0.2443 - e_loss: 0.0226 - reg_loss: 0.2217 - val_RMSE/at: 0.0881 - val_e_loss: 0.0969\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 6ms/step - tot_st: 700.0000 - RMSE/at: 0.0384 - loss: 0.2442 - e_loss: 0.0226 - reg_loss: 0.2216 - val_RMSE/at: 0.0415 - val_e_loss: 0.0209\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 7ms/step - tot_st: 800.0000 - RMSE/at: 0.0387 - loss: 0.2440 - e_loss: 0.0224 - reg_loss: 0.2216 - val_RMSE/at: 0.0612 - val_e_loss: 0.0498\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 1s 6ms/step - tot_st: 900.0000 - RMSE/at: 0.0379 - loss: 0.2432 - e_loss: 0.0216 - reg_loss: 0.2216 - val_RMSE/at: 0.0537 - val_e_loss: 0.0382\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 7ms/step - tot_st: 1000.0000 - RMSE/at: 0.0389 - loss: 0.2442 - e_loss: 0.0226 - reg_loss: 0.2216 - val_RMSE/at: 0.0438 - val_e_loss: 0.0272\n"
     ]
    }
   ],
   "source": [
    "!cd {panna_cmdir+'/doc/tutorial/'}; python {panna_cmdir+'/src/panna/train.py'} --config ./input_files/my_adv_train.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the RMSE is reported, and the loss including the regularization loss. In addition, we see that at the end of each epoch the same values are reported for the validation set. All this is very important to monitor complex training cases and find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 — Computing Gvectors during training\n",
    "\n",
    "Finally, we want to show a training option that can be very useful to iterate parameters quickly, or in cases of very large datasets.\n",
    "As we have mentioned, precomputing the derivatives of the descriptors can take up a large amount of space, and create a large dataset that needs to be loaded to memory while training, possibly a number of times. While this is often worth the savings in computation time (since examples are used many times and the descriptors are always the same), it can be a problem in a few cases, like when the training set is very large (or I/O limited in the machine used for training), or if a some quick training needs to be done to test descriptor parameters, and we do not want to create multiple large copies of the descritors for single use.\n",
    "\n",
    "For all these cases, it is now possible in PANNA to compute the descriptors from the example files while we are training the network. This is considerably more computationally expensive, but feasible on last generation GPUs. More specifically, please note that the first training steps can be especially slow, because the code needs to be optimized for different inputs. As the train progresses, you will typically see a speedup.\n",
    "\n",
    "To enable this option, we need to set the option ``input_format`` to ``example`` in the io_information (the default was ``tfr``). Also, we need to specify the parameters of the descriptors, so we can use the keyword ``gvect_ini`` and pass the same input file as we have prepared for the precomputation. Now we can simply indicate the ``data_dir`` where the ``.example`` files are located, and we can start the training.\n",
    "\n",
    "Let us look at a sample training file that reuses the data and parameters we used in the rest of the tutorial (to keep the training light, we will not use forces in this example, although this is not the most common use case), then run a short training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IO_INFORMATION]\r\n",
      "data_dir = ./tutorial_data/simulations\r\n",
      "train_dir = ./my_train_fromex\r\n",
      "input_format = example\r\n",
      "gvect_ini = ./input_files/mygvect_sample.ini\r\n",
      "log_frequency = 10\r\n",
      "save_checkpoint_steps = 100\r\n",
      "\r\n",
      "[DATA_INFORMATION]\r\n",
      "atomic_sequence = H, C, O, N\r\n",
      "output_offset = -13.62, -1029.41, -2041.84, -1484.87\r\n",
      "\r\n",
      "[TRAINING_PARAMETERS]\r\n",
      "batch_size = 5\r\n",
      "learning_rate = 0.01\r\n",
      "steps_per_epoch = 20\r\n",
      "max_epochs = 5\r\n",
      "\r\n",
      "[DEFAULT_NETWORK]\r\n",
      "g_size = 384\r\n",
      "architecture = 128:32:1\r\n",
      "trainable = 1:1:1\r\n"
     ]
    }
   ],
   "source": [
    "!cat {panna_cmdir+'/doc/tutorial/input_files/mytrain_fromex.ini'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-25 11:11:42.626440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-04-25 11:11:42.626521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-04-25 11:11:42.626542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO - \n",
      "    ____   _    _   _ _   _    _           \n",
      "   |  _ \\ / \\  | \\ | | \\ | |  / \\     \n",
      "   | |_) / _ \\ |  \\| |  \\| | / _ \\     \n",
      "   |  __/ ___ \\| |\\  | |\\  |/ ___ \\    \n",
      "   |_| /_/   \\_\\_| \\_|_| \\_/_/   \\_\\ \n",
      "\n",
      " Properties from Artificial Neural Network Architectures\n",
      "\n",
      "INFO - reading ./input_files/mytrain_fromex.ini\n",
      "INFO - Found a default network!\n",
      "INFO - This network size will be used as default for all species unless specified otherwise\n",
      "INFO - Radial Gaussian centers are set by Rs0_rad, Rc_rad, RsN_rad\n",
      "INFO - Angular descriptor centers are set by ThetasN\n",
      "INFO - Radial-angular Gaussian centers are set by Rs0_ang, Rc_ang, RsN_ang\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 9s 132ms/step - tot_st: 20.0000 - MAE/at: 366.1680\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 0s 19ms/step - tot_st: 40.0000 - MAE/at: 375.8267\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 0s 19ms/step - tot_st: 60.0000 - MAE/at: 359.5655\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 0s 19ms/step - tot_st: 80.0000 - MAE/at: 363.0202\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 0s 22ms/step - tot_st: 100.0000 - MAE/at: 355.8039\n"
     ]
    }
   ],
   "source": [
    "!cd {panna_cmdir+'/doc/tutorial/'}; python {panna_cmdir+'/src/panna/train.py'} --config ./input_files/mytrain_fromex.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to cleanup the tutorial directory\n",
    "!cd {panna_cmdir+'/doc/tutorial'}; rm -rf my_train_force my_adv_train tf.log my_train_fromex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
